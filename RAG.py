from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load embedding model (Sentence-Transformer for embedding generation)
embed_model = SentenceTransformer("all-MiniLM-L6-v2")  # A small, efficient model for generating sentence embeddings

# Example knowledge base (documents for retrieval)
documents = [
    "RAG is a technique to enhance LLMs with external knowledge.",  # Document 1
    "Vector similarity search helps retrieve relevant documents efficiently.",  # Document 2
    "Knowledge retrieval improves model responses by adding context."  # Document 3
]

# Compute embeddings for the documents
doc_embeddings = np.array([embed_model.encode(doc) for doc in documents])  # Each document is converted to a vector (embedding)

# Function to retrieve relevant documents based on a query
def retrieve_docs(query, doc_embeddings, docs, top_k=2):
    """
    Retrieve the top-k most relevant documents from a knowledge base based on the query.
    
    Args:
    - query: The input query string.
    - doc_embeddings: Embeddings of the documents.
    - docs: The list of documents.
    - top_k: Number of top documents to retrieve.
    
    Returns:
    - A list of the most relevant documents based on cosine similarity.
    """
    query_embedding = embed_model.encode(query).reshape(1, -1)  # Convert the query into an embedding
    similarities = cosine_similarity(query_embedding, doc_embeddings)  # Calculate cosine similarity between query and all documents
    top_indices = np.argsort(similarities[0])[-top_k:][::-1]  # Get the indices of the top-k most similar documents
    return [docs[i] for i in top_indices]  # Return the top-k documents

# Load an autoregressive model (GPT-2 for text generation)
tokenizer = AutoTokenizer.from_pretrained("gpt2")  # Load the tokenizer for GPT-2
model = AutoModelForCausalLM.from_pretrained("gpt2")  # Load the GPT-2 model for causal language modeling (text generation)

# Define a query and retrieve relevant documents from the knowledge base
query = "How does RAG work?"  # Define the query that needs additional context from the knowledge base
retrieved_docs = retrieve_docs(query, doc_embeddings, documents)  # Retrieve relevant documents based on the query

# Combine the retrieved documents into a context string
retrieved_text = "\n".join(retrieved_docs)  # Join the top-k retrieved documents into a single string (context)
final_prompt = f"Context:\n{retrieved_text}\n\nQuestion: {query}\nAnswer:"  # Format the prompt for GPT-2 with context and query

# Tokenize the final prompt and generate a response using GPT-2
input_ids = tokenizer.encode(final_prompt, return_tensors="pt")  # Tokenize the prompt to prepare it for the model
output_ids = model.generate(input_ids, max_length=100, num_return_sequences=1)  # Generate a response from GPT-2

# Decode the generated response and print the result
response = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Decode the token IDs into a human-readable response
print(response)  # Print the response generated by the model
